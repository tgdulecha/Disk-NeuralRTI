<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 1000px;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 0px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>

<head>
    <title>Disk-NeuralRTI: Optimized NeuralRTI Relighting through Knowledge Distillations</title>
    <meta property="og:title" content="Self-calibrating Deep Photometric Stereo Networks, In CVPR 2019." />
</head>

<body>
    <br>
    <center>
        <span style="font-size:42px">Disk-NeuralRTI: Optimized NeuralRTI Relighting through Knowledge
            Distillation</span><br>
        <table align=center width=900px>
            <tr>
                <td align=center width=900px>
                    <span style="font-size:22px"><a href="https://tgdulecha.github.io/">Tinsae G.
                            Dulecha<sup>1</sup></a></span> &emsp;
                    <span style="font-size:22px"><a href="https://www.di.univr.it/?ent=persona&id=77290&lang=en">Leonardo Righetto<sup>1</sup></a></span> &emsp;
                    <span style="font-size:22px"><a href="https://www.crs4.it/peopledetails/ruggero/pintus-ruggero/">Ruggero Pintus Shi<sup>2</sup></a></span> &emsp;
                    <span style="font-size:22px"><a href="https://www.crs4.it/peopledetails/8/enrico-gobbetti/">Enrico Gobbetti<sup>2</sup></a></span> &emsp;
                    <span style="font-size:22px"><a href="http://www.andreagiachetti.it/andrea/">Andrea
                            Giachetti<sup>1</sup></a></span>
                </td>
            </tr>
        </table>
        <table align=center width=800px>
            <td align=center width=300px>
                <span style="font-size:21px"><sup>1</sup>University of Verona,Italy &emsp; <sup>2</sup>CRS4, Italy</span> <br>
            </td>
        
        </table>
        <br>
        <table align=center width=900px>
            <tr>
                <td align=center width=900px>
                    <center>
                        <span style="font-size:22px"><a href="https://github.com/tgdulecha/DiskNeuralRTI-code.git" target="_blank">Code
                                [PyTorch]</a></span> &emsp; &emsp;
                        <span style="font-size:22px"><a href="" target="_blank">Paper [STAG2024]</a> </span> &emsp; &emsp;
                        <span style="font-size:22px"><a href="" target="_blank"> Presentation Slide [Coming soon!]</a></span>
                    </center>
                </td>
            </tr>
        </table>
    </center>
    <br>


    <hr>

    <table align=center width=900px>
        <center>
            <h1>Abstract</h1>
        </center>
        <p align="justify">
            Relightable images created from Multi-Light Image Collections (MLICs) are among the most employed models for
            interactive object exploration in cultural heritage (CH). In recent years, neural representations have been
            shown to produce higher-quality images at similar storage costs to the more classic analytical models such
            as Polynomial Texture Maps (PTM) or Hemispherical Harmonics (HSH). However, the Neural RTI models proposed
            in the literature perform the image relighting with decoder networks with a high number of parameters,
            making decoding slower than for classical methods. Despite recent efforts targeting model reduction and
            multi-resolution adaptive rendering, exploring high-resolution images, especially on high-pixel-count
            displays, still requires significant resources and is only achievable through progressive rendering in
            typical setups. In this work, we show how, by using knowledge distillation from an original (teacher) Neural
            RTI network, it is possible to create a more efficient RTI decoder (student network). We evaluated the
            performance of the network compression approach on existing RTI relighting benchmarks, including both
            synthetic and real datasets, and on novel acquisitions of high-resolution images. Experimental results show
            that we can keep the student prediction close to the teacher with up to 80% parameter reduction and almost
            ten times faster rendering when embedded in an online viewer.

        </p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h1>Video</h1>
        </center>
        <center>
            <iframe width="800" height="450" src="https://www.youtube.com/embed/5_puqhNvOtQ?si=Pj3AhfpR8YF_AAgY"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        
        </center>
    </table>
    <hr>

    <table align=center>
        <center>
            <h1>Method</h1>
        </center>
        <tr>
            <td align=center width=1000px>
                <img class="round" style="width:1000px" src="./images/DiskNeuralRTI_scheme.png"></img>
            </td>
        </tr>
        <tr>
            <td>
                <p><br>Figure 1: Network architecture for original NeuralRTI (top) and Disk-
                    NeuralRTI (bottom)</p>
            </td>
        </tr>
    </table>
    <hr>


    <table align=center width=1000px>
        <tr>
            <td width=400px>
                <left>
                    <center>
                        <h1>Acknowledgments</h1>
                    </center>
                    <p align="justify">
                        This study was partially funded by the consortium iNEST funded by the EU Next-GenerationEU PNNR
                        M4C2
                        Inv1.5 â€“ D.D. 1058 23/06/2022, ECS00000043 and by the project REFLEX (PRIN2022) funded by the EU
                        Next-GenerationEU PNRR M4C2 Inv. 1.1. EG and RP also acknowledge the contribution of Sardinian
                        Regional Authorities under project XDATA (RAS Art9 LR 20/2015). We thank Prof. Attilio
                        Mastrocinque
                        and the Civic Archaeological Museum of Milan for access to the lead sheet and the National
                        Archaeological Museum of Cagliari for access to the retablos. We thank Fabio Bettio (CRS4),
                        Fabio
                        Marton (CRS4), and Federico Ponchio (ISTI-CNR) for their work in developing OpenLime and for the
                        integration of Neural RTI rendering within the framework.
                        We acknowledge the CINECA award under the ISCRA initiative (BERET project) for the availability
                        of
                        high-performance computing resources and support.</p>
                </left>
            </td>
        </tr>
    </table>
    <br>

    <p style="text-align:center;font-size:16px;">
        Webpage template borrowed from <a href="https://richzhang.github.io/splitbrainauto/">Split-Brain Autoencoders,
            CVPR 2017</a>.
    </p>
</body>

</html>