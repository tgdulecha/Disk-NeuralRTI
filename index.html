<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 1000px;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 0px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>

<head>
    <title>Disk-NeuralRTI: Optimized NeuralRTI Relighting through Knowledge Distillations</title>
    <meta property="og:title" content="Self-calibrating Deep Photometric Stereo Networks, In CVPR 2019." />
</head>

<body>
    <br>
    <center>
        <span style="font-size:42px">Disk-NeuralRTI: Optimized NeuralRTI Relighting through Knowledge
            Distillation</span><br>
        <table align=center width=900px>
            <tr>
                <td align=center width=900px>
                    <span style="font-size:22px"><a href="https://tgdulecha.github.io/" target="_blank">Tinsae G.
                            Dulecha<sup>1</sup></a></span> &emsp;
                    <span style="font-size:22px"><a href="https://www.di.univr.it/?ent=persona&id=77290&lang=en"
                            target="_blank">Leonardo
                            Righetto<sup>1</sup></a></span> &emsp;
                    <span style="font-size:22px"><a href="https://www.crs4.it/peopledetails/ruggero/pintus-ruggero/"
                            target="_blank">Ruggero Pintus
                            <sup>2</sup></a></span> &emsp;
                    <span style="font-size:22px"><a href="https://www.crs4.it/peopledetails/8/enrico-gobbetti/"
                            target="_blank">Enrico
                            Gobbetti<sup>2</sup></a></span> &emsp;
                    <span style="font-size:22px"><a href="http://www.andreagiachetti.it/andrea/" target="_blank">Andrea
                            Giachetti<sup>1</sup></a></span>
                </td>
            </tr>
        </table>
        <table align=center width=800px>
            <td align=center width=300px>
                <span style="font-size:21px"><sup>1</sup>University of Verona,Italy &emsp; <sup>2</sup>CRS4,
                    Italy</span> <br>
            </td>

        </table>
        <br>
        <table align=center width=900px>
            <tr>
                <td align=center width=900px>
                    <center>
                        <span style="font-size:22px"><a href="https://github.com/tgdulecha/DiskNeuralRTI-code.git"
                                target="_blank">Code
                                (will be released)</a></span> &emsp; &emsp;
                        <span style="font-size:22px"><a href="" target="_blank">Paper [STAG2024]</a> </span> &emsp;
                        &emsp;
                        <span style="font-size:22px"><a href="" target="_blank"> Presentation Slide [Coming
                                soon!]</a></span>
                    </center>
                </td>
            </tr>
        </table>
    </center>
    <br>


    <hr>

    <table align=center width=900px>
        <center>
            <h1>Abstract</h1>
        </center>
        <p align="justify">
            Relightable images created from Multi-Light Image Collections (MLICs) are among the most employed models for
            interactive object exploration in cultural heritage (CH). In recent years, neural representations have been
            shown to produce higher-quality images at similar storage costs to the more classic analytical models such
            as Polynomial Texture Maps (PTM) or Hemispherical Harmonics (HSH). However, the Neural RTI models proposed
            in the literature perform the image relighting with decoder networks with a high number of parameters,
            making decoding slower than for classical methods. Despite recent efforts targeting model reduction and
            multi-resolution adaptive rendering, exploring high-resolution images, especially on high-pixel-count
            displays, still requires significant resources and is only achievable through progressive rendering in
            typical setups. In this work, we show how, by using knowledge distillation from an original (teacher) Neural
            RTI network, it is possible to create a more efficient RTI decoder (student network). We evaluated the
            performance of the network compression approach on existing RTI relighting benchmarks, including both
            synthetic and real datasets, and on novel acquisitions of high-resolution images. Experimental results show
            that we can keep the student prediction close to the teacher with up to 80% parameter reduction and almost
            ten times faster rendering when embedded in an online viewer.

        </p>
    </table>
    <hr>

    <table align=center width=900px>
        <center>
            <h1>Video</h1>
        </center>
        <center>
        
            <iframe width="900" height="520" src="https://www.youtube.com/embed/VPKC2je0BXs?si=r6tzqaB6p__lMqNB"
                title="YouTube video player" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

        </center>
    </table>
    <hr>

    <table align=center>
        <center>
            <h1>Method</h1>
        </center>
        <tr>
            <figure>

                <img class="round" style="width:1000px" src="./images/NeuralRTI-scheme.png" />
                <figcaption>
                    <center>
                        (a) NeuralRTI scheme. The encoder has three hidden layers and the decoder has two hidden lay-
                        ers. Each hidden layer contains 50 units. The encoder receives RTI pixel data and produces a
                        9-dimensional code. The decoder concatenates the code vector with the light direction and
                        outputs
                        a single RGB value. </center>
                </figcaption>
            </figure>
        </tr>
        <tr>
            <td align=center width=1000px>
                <figure>
                    <img class="round" style="width:1000px" src="./images/DiskNeuralRTI_scheme.png" />
                    <figcaption>
                        <center>(b) Disk-NeuralRTI. The encoder has the same architecture for student and teacher
                            networks.
                            The student network decoder contains two layers, each with an R number of units. We tested
                            it
                            with R values of 10 and 20. </center>
                    </figcaption>
                </figure>

            </td>
        </tr>

        <tr>
            <td>
                <p><br>Figure 1: Network architecture for original NeuralRTI (a) and Disk-
                    NeuralRTI (b)</p>
            </td>
        </tr>
    </table>
    <hr>


    <table align=center width=1000px>
        <tr>
            <td width=400px>
                <left>
                    <center>
                        <h1>Acknowledgments</h1>
                    </center>
                    <p align="justify">
                        This study was partially funded by the consortium iNEST funded by the EU Next-GenerationEU PNNR
                        M4C2
                        Inv1.5 â€“ D.D. 1058 23/06/2022, ECS00000043 and by the project REFLEX (PRIN2022) funded by the EU
                        Next-GenerationEU PNRR M4C2 Inv. 1.1. EG and RP also acknowledge the contribution of Sardinian
                        Regional Authorities under project XDATA (RAS Art9 LR 20/2015). We thank Prof. Attilio
                        Mastrocinque
                        and the Civic Archaeological Museum of Milan for access to the lead sheet and the National
                        Archaeological Museum of Cagliari for access to the retablos. We thank Fabio Bettio (CRS4),
                        Fabio
                        Marton (CRS4), and Federico Ponchio (ISTI-CNR) for their work in developing OpenLime and for the
                        integration of Neural RTI rendering within the framework. We acknowledge ISCRA for awarding this
                        project access to the LEONARDO supercomputer, owned by the EuroHPC Joint Undertaking, hosted by
                        CINECA (Italy) .
                    </p>
                </left>
            </td>
        </tr>
    </table>
    <br>

    <!-- <p style="text-align:center;font-size:16px;">
        Webpage template borrowed from <a href="https://richzhang.github.io/splitbrainauto/">Split-Brain Autoencoders,
            CVPR 2017</a>.
    </p> -->
</body>

</html>